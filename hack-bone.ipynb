{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT1BKl2tvsfw",
        "outputId": "d66c364a-4d3b-4d9d-cde7-43b27c7d1710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "ktYufzh-vu__",
        "outputId": "6a593618-6052-4ce1-b3c0-534fa68cc7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7749fbd5-ecc4-49bc-8945-7a213772b6ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7749fbd5-ecc4-49bc-8945-7a213772b6ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"westoncadena\",\"key\":\"72f09f2daaf65e5c25c7bd7c2e6d5b5f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/Hack-Bone/\"  # Change this to your desired folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxM6PhXYczVt",
        "outputId": "f26013d7-1ccb-4904-dc28-76f030574b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "\n",
        "# Set file permissions\n",
        "os.chmod('/root/.kaggle/kaggle.json', 600)"
      ],
      "metadata": {
        "id": "zgvrkdXHvvPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Download and unzip the dataset\n",
        "dataset = 'mariusmarin/bs-80k'\n",
        "!kaggle datasets download -d {dataset}\n",
        "\n",
        "# Define the zip file\n",
        "zip_file = 'bs-80k.zip'\n",
        "\n",
        "# Extract all files from the zip archive\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall('bs-80k')  # Extract to 'bs-80k' folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAtxkWTXHK3p",
        "outputId": "8c67ef6e-8cd2-44a6-af82-61478e22e8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mariusmarin/bs-80k\n",
            "License(s): MIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mSegResRF-SPECT Implementation\n",
        "*Based on \"Novel Joint Classification Model\" (Current Medical Imaging, 2024, Volume 20)*\n"
      ],
      "metadata": {
        "id": "gIplP2GinpJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Configuration"
      ],
      "metadata": {
        "id": "5JCE3ivTnyrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ResNet34_Weights\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# from sklearn.metrics import accuracy_score, sensitivity_score, specificity_score, f1_score, confusion_matrix\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'data_dir': 'bs-80k/temp',\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.0001,\n",
        "    'epochs': 100, #100\n",
        "    'num_workers': 2,\n",
        "    'image_size': 256,\n",
        "    'crop_size': 224,\n",
        "    'train_test_split': 0.8,\n",
        "    'k_folds': 5, #5\n",
        "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "}\n",
        "\n",
        "# Define the selected regions (according to the paper)\n",
        "SELECTED_REGIONS = [\n",
        "    # 'testANT',        # 1\n",
        "    # 'testPOST',\n",
        "    'headANT',\n",
        "    'vertebraANT',    # 5\n",
        "    'chestLANT',      # 8\n",
        "    'chestRANT',      # 9\n",
        "    'pelvisANT',      # 10\n",
        "    'kneeLANT',       # 12\n",
        "    'kneeRANT'        # 13\n",
        "]\n"
      ],
      "metadata": {
        "id": "YQHXFosNn_j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset and DataLoader"
      ],
      "metadata": {
        "id": "wWFeDUnpoKLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneScanDataset(Dataset):\n",
        "    def __init__(self, data_dir, region, file_list, labels, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.region = region\n",
        "        self.file_list = file_list\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.region, self.file_list[idx])\n",
        "\n",
        "        # Handle potential file not found errors\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            print(f\"Error loading image: {img_path}\")\n",
        "            # Return a placeholder black image\n",
        "            image = Image.new('RGB', (CONFIG['image_size'], CONFIG['image_size']), color=0)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "f79BprKUoYd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Architecture"
      ],
      "metadata": {
        "id": "E_WWe48hoWOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        base = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
        "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
        "        self.embedding = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, 2)  # Add classification layer\n",
        "\n",
        "    def forward(self, x, return_embedding=False):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        embedding = self.embedding(x)\n",
        "        if return_embedding:\n",
        "            return embedding\n",
        "        out = self.classifier(embedding)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Y1KGGYFCoWD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Preparation Function"
      ],
      "metadata": {
        "id": "_nArdwqoop9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data_dir):\n",
        "    \"\"\"\n",
        "    Prepare the data by loading file names and labels from the region text files.\n",
        "    Returns a dictionary with region names as keys and (file_list, labels) as values.\n",
        "    \"\"\"\n",
        "    data_dict = {}\n",
        "    whole_body_labels = {}\n",
        "    whole_body_dict = {}\n",
        "\n",
        "    # Load the wholeBodyANT labels\n",
        "    whole_body_path = os.path.join(data_dir, 'wholeBodyANT')\n",
        "    if os.path.exists(whole_body_path):\n",
        "        label_file_path = os.path.join(whole_body_path, 'wholeBodyANT.txt')\n",
        "        if os.path.exists(label_file_path):\n",
        "            print(f\"Found label file for whole body: {label_file_path}\")\n",
        "            with open(label_file_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 2:\n",
        "                        img_name = parts[0]\n",
        "                        label = int(parts[1])\n",
        "                        whole_body_labels[img_name] = label\n",
        "        else:\n",
        "            print(f\"Warning: No label file found for whole body at {label_file_path}.\")\n",
        "    else:\n",
        "        print(\"Warning: wholeBodyANT directory not found.\")\n",
        "\n",
        "    for region in SELECTED_REGIONS:\n",
        "        region_path = os.path.join(data_dir, region)\n",
        "\n",
        "        if not os.path.exists(region_path):\n",
        "            print(f\"Warning: Region directory {region_path} does not exist.\")\n",
        "            continue\n",
        "\n",
        "        # Read the region-specific label file\n",
        "        label_file_path = os.path.join(region_path, f\"{region}.txt\")\n",
        "        if not os.path.exists(label_file_path):\n",
        "            print(f\"Warning: Label file {label_file_path} does not exist.\")\n",
        "            continue\n",
        "        else:\n",
        "            print(f\"Found label file: {label_file_path}\")\n",
        "\n",
        "        # Read the region label file\n",
        "        region_image_labels = {}\n",
        "        with open(label_file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 2:\n",
        "                    img_name = parts[0]\n",
        "                    label = int(parts[1])\n",
        "                    region_image_labels[img_name] = label\n",
        "\n",
        "        # Get all available image files\n",
        "        all_files = [f for f in os.listdir(region_path) if f.endswith('.jpg')]\n",
        "\n",
        "        # Filter to include only images that have labels in both region-specific and wholeBody labels\n",
        "        file_list = []\n",
        "        labels = []\n",
        "\n",
        "        for img_file in all_files:\n",
        "            # Check if the image file is in both the region label and the wholeBody labels\n",
        "            if img_file in region_image_labels and img_file in whole_body_labels:\n",
        "                file_list.append(img_file)\n",
        "                # Use the label from the wholeBodyANT file as the label for this image\n",
        "                labels.append(region_image_labels[img_file])\n",
        "                # Add the image to whole_body_labels if it's part of the region\n",
        "                whole_body_dict[img_file] = whole_body_labels[img_file]\n",
        "\n",
        "        if len(file_list) == 0:\n",
        "            print(f\"Warning: No valid labeled images found for region {region}.\")\n",
        "            continue\n",
        "\n",
        "        data_dict[region] = (file_list, labels)\n",
        "\n",
        "    return data_dict, whole_body_dict\n"
      ],
      "metadata": {
        "id": "o8vVOeeMoqeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_whole_body_dict(whole_body_dict):\n",
        "    \"\"\"\n",
        "    Analyze the labels in whole_body_dict and output important statistics.\n",
        "    \"\"\"\n",
        "    if not whole_body_dict:\n",
        "        print(\"Warning: whole_body_dict is empty.\")\n",
        "        return\n",
        "\n",
        "    # Count the occurrences of each label in whole_body_dict\n",
        "    label_counts = {}\n",
        "    for label in whole_body_dict.values():\n",
        "        label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    # Print the total number of samples\n",
        "    total_samples = len(whole_body_dict)\n",
        "    print(f\"Total number of samples in whole_body_dict: {total_samples}\")\n",
        "\n",
        "    # Print the label distribution\n",
        "    print(\"Label distribution:\")\n",
        "    for label, count in label_counts.items():\n",
        "        print(f\"  Label {label}: {count} samples\")\n",
        "\n",
        "\n",
        "data_dir = \"bs-80k/temp\"  # Update this with the actual data directory\n",
        "data_dict, whole_body_dict = prepare_data(data_dir)\n",
        "\n",
        "# Now analyze the whole_body_dict\n",
        "analyze_whole_body_dict(whole_body_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biC4s-w7yrLJ",
        "outputId": "4f2d22ca-68cb-440c-e18c-5dc699258af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: wholeBodyANT directory not found.\n",
            "Warning: Region directory bs-80k/temp/testANT does not exist.\n",
            "Warning: Region directory bs-80k/temp/testPOST does not exist.\n",
            "Warning: whole_body_dict is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Feature Extractor Training"
      ],
      "metadata": {
        "id": "Hvb5Rdh7oyQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_feature_extractors(data_dict, config):\n",
        "    \"\"\"\n",
        "    Train a feature extractor for each region.\n",
        "    Returns a dictionary of trained models.\n",
        "    \"\"\"\n",
        "    overall_start = time.time()\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = config['device']\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Warning: No GPU found. Training on CPU.\")\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.RandomCrop(config['crop_size']),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.CenterCrop(config['crop_size']),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    models_dict = {}\n",
        "\n",
        "    for region in SELECTED_REGIONS:\n",
        "        if region not in data_dict:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTraining feature extractor for {region}...\")\n",
        "        region_start = time.time()\n",
        "\n",
        "        files, labels = data_dict[region]\n",
        "        split_idx = int(len(files) * config['train_test_split'])\n",
        "        train_files, train_labels = files[:split_idx], labels[:split_idx]\n",
        "        test_files, test_labels = files[split_idx:], labels[split_idx:]\n",
        "\n",
        "        # Datasets + Loaders\n",
        "        train_dataset = BoneScanDataset(config['data_dir'], region, train_files, train_labels, train_transform)\n",
        "        test_dataset = BoneScanDataset(config['data_dir'], region, test_files, test_labels, val_transform)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
        "\n",
        "        # Model, loss, optimizer\n",
        "        model = FeatureExtractor().to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "\n",
        "        best_acc = -1.0\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            train_loss /= len(train_loader.dataset)\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                model.eval()\n",
        "                val_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in test_loader:\n",
        "                        inputs, targets = inputs.to(device), targets.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, targets)\n",
        "\n",
        "                        val_loss += loss.item() * inputs.size(0)\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        total += targets.size(0)\n",
        "                        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "                val_loss /= len(test_loader.dataset)\n",
        "                val_acc = correct / total\n",
        "\n",
        "                print(f\"Region: {region}, Epoch: {epoch+1}/{config['epochs']}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    print(f\"Saving best model for {region} at epoch {epoch+1}, val_acc = {val_acc:.4f}\")\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    torch.save(model.state_dict(), os.path.join(save_dir, f\"resnet34_{region}_best.pth\"))\n",
        "\n",
        "        # Load and strip the classifier for feature extraction\n",
        "        model.load_state_dict(torch.load(os.path.join(save_dir, f\"resnet34_{region}_best.pth\"), map_location=device))\n",
        "        model.classifier = nn.Identity()  # Remove the classifier for final use\n",
        "        models_dict[region] = model\n",
        "\n",
        "        region_end = time.time()\n",
        "        print(f\"Finished training for {region} in {(region_end - region_start):.2f} seconds.\")\n",
        "\n",
        "    overall_end = time.time()\n",
        "    print(f\"\\nAll regions processed in {(overall_end - overall_start):.2f} seconds.\")\n",
        "\n",
        "    return models_dict"
      ],
      "metadata": {
        "id": "h6lCBZvFoyIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Feature Extraction"
      ],
      "metadata": {
        "id": "iDWLwF0DpBJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data_dict, models_dict, config):\n",
        "    \"\"\"\n",
        "    Extract features for all regions using trained models.\n",
        "    Returns a dictionary of features and labels for each region.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = config['device']\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Warning: No GPU found. Training on CPU.\")\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.CenterCrop(config['crop_size']),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    features_dict = {}\n",
        "\n",
        "    for region in SELECTED_REGIONS:\n",
        "        if region not in data_dict or region not in models_dict:\n",
        "            continue\n",
        "\n",
        "        print(f\"Extracting features for {region}...\")\n",
        "\n",
        "        files, labels = data_dict[region]\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = BoneScanDataset(config['data_dir'], region, files, labels, val_transform)\n",
        "\n",
        "        # Create dataloader\n",
        "        dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        model = models_dict[region].to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Extract features\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        all_files = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                all_features.append(outputs.cpu().numpy())\n",
        "                all_labels.append(targets.numpy())\n",
        "\n",
        "                # Keep track of which files we're processing\n",
        "                start_idx = batch_idx * config['batch_size']\n",
        "                end_idx = min(start_idx + config['batch_size'], len(files))\n",
        "                all_files.extend(files[start_idx:end_idx])\n",
        "\n",
        "        # Concatenate all batches\n",
        "        features = np.vstack(all_features)\n",
        "        labels = np.concatenate(all_labels)\n",
        "\n",
        "        features_dict[region] = (features, labels, all_files)\n",
        "\n",
        "    return features_dict"
      ],
      "metadata": {
        "id": "F4FS8J17pIXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Random Forest Classifier Training"
      ],
      "metadata": {
        "id": "mfi5Qc8PpIyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_random_forest(features_dict, whole_body_dict, config):\n",
        "    start_time = time.time()\n",
        "\n",
        "    region_files = {\n",
        "        region: [os.path.splitext(f)[0] for f in features_dict[region][2]]\n",
        "        for region in features_dict\n",
        "    }\n",
        "\n",
        "    common_identifiers = set(region_files[list(region_files.keys())[0]])\n",
        "    for region in region_files:\n",
        "        common_identifiers &= set(region_files[region])\n",
        "\n",
        "    if not common_identifiers:\n",
        "        print(\"Error: No common samples found across all regions.\")\n",
        "        return None, (0, 0, 0, 0, 0)\n",
        "\n",
        "    common_identifiers = sorted(list(common_identifiers))\n",
        "    print(f\"Found {len(common_identifiers)} common samples across all regions.\")\n",
        "\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for identifier in common_identifiers:\n",
        "        sample_feature = []\n",
        "\n",
        "        for region in features_dict:\n",
        "            features, _, files = features_dict[region]\n",
        "            file_to_idx = {os.path.splitext(f)[0]: i for i, f in enumerate(files)}\n",
        "\n",
        "            if identifier in file_to_idx:\n",
        "                idx = file_to_idx[identifier]\n",
        "                sample_feature.append(features[idx])\n",
        "            else:\n",
        "                sample_feature = None\n",
        "                break\n",
        "\n",
        "        if sample_feature is not None:\n",
        "            file_name = f\"{identifier}.jpg\"\n",
        "            if file_name in whole_body_dict:\n",
        "                full_feature = np.concatenate(sample_feature)\n",
        "                all_features.append(full_feature)\n",
        "                all_labels.append(whole_body_dict[file_name])\n",
        "\n",
        "    if not all_features:\n",
        "        print(\"Error: No valid samples with full feature vectors.\")\n",
        "        return None, (0, 0, 0, 0, 0)\n",
        "\n",
        "    X = np.vstack(all_features)\n",
        "    y = np.array(all_labels)\n",
        "\n",
        "    print(f\"Total samples: {len(y)}\")\n",
        "    print(f\"Label distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
        "\n",
        "    if len(np.unique(y)) < 2:\n",
        "        print(\"Warning: Only one class present in data. Cannot train classifier.\")\n",
        "        return None, (0, 0, 0, 0, 0)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=min(config['k_folds'], len(y)), shuffle=True, random_state=42)\n",
        "    accuracies, sensitivities, specificities, f1_scores, aucs = [], [], [], [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        print(f\"\\nTraining fold {fold+1}/{kf.get_n_splits()}...\")\n",
        "        fold_start = time.time()\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        if len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2:\n",
        "            print(f\"Warning: Fold {fold+1} has insufficient class diversity. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_proba = clf.predict_proba(X_val)[:, 1]\n",
        "        y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_val, y_pred, labels=[0, 1])\n",
        "        if cm.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "        else:\n",
        "            tn = fp = fn = tp = 0\n",
        "            if np.sum(y_val == 0) == len(y_val):\n",
        "                tn = len(y_val)\n",
        "            elif np.sum(y_val == 1) == len(y_val):\n",
        "                tp = len(y_val)\n",
        "\n",
        "        sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        f1 = f1_score(y_val, y_pred, zero_division=0)\n",
        "\n",
        "        if len(np.unique(y_val)) == 2:\n",
        "            auc = roc_auc_score(y_val, y_proba)\n",
        "        else:\n",
        "            print(f\"Skipping AUC for Fold {fold+1} — only one class in validation.\")\n",
        "            auc = 0\n",
        "\n",
        "        accuracies.append(acc)\n",
        "        sensitivities.append(sen)\n",
        "        specificities.append(spe)\n",
        "        f1_scores.append(f1)\n",
        "        aucs.append(auc)\n",
        "\n",
        "        fold_end = time.time()\n",
        "        print(f\"Fold {fold+1} - Acc: {acc:.4f}, Sen: {sen:.4f}, Spe: {spe:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "        print(f\"Fold {fold+1} completed in {(fold_end - fold_start):.2f} seconds.\")\n",
        "\n",
        "    if not accuracies:\n",
        "        print(\"No valid folds were processed. Cannot compute metrics.\")\n",
        "        return None, (0, 0, 0, 0, 0)\n",
        "\n",
        "    avg_acc = np.mean(accuracies)\n",
        "    avg_sen = np.mean(sensitivities)\n",
        "    avg_spe = np.mean(specificities)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "    avg_auc = np.mean(aucs)\n",
        "\n",
        "    print(f\"\\nAverage metrics - Acc: {avg_acc:.4f}, Sen: {avg_sen:.4f}, Spe: {avg_spe:.4f}, F1: {avg_f1:.4f}, AUC: {avg_auc:.4f}\")\n",
        "\n",
        "    final_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    final_clf.fit(X, y)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTotal training time: {(end_time - start_time):.2f} seconds.\")\n",
        "\n",
        "    return final_clf, (avg_acc, avg_sen, avg_spe, avg_f1, avg_auc)"
      ],
      "metadata": {
        "id": "s2v09mnupTfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Main Training Pipeline"
      ],
      "metadata": {
        "id": "gQpnXxOapTyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training pipeline\n",
        "def train_mSegResRF_SPECT():\n",
        "    \"\"\"\n",
        "    Main function to train the mSegResRF-SPECT model.\n",
        "    \"\"\"\n",
        "    print(\"Starting mSegResRF-SPECT training pipeline...\")\n",
        "\n",
        "    # 1. Prepare data\n",
        "    print(\"Preparing data...\")\n",
        "    data_dict, whole_body_dict = prepare_data(CONFIG['data_dir'])\n",
        "\n",
        "    analyze_whole_body_dict(whole_body_dict)\n",
        "\n",
        "    # 2. Train feature extractors for each region\n",
        "    print(\"Training feature extractors...\")\n",
        "    models_dict = train_feature_extractors(data_dict, CONFIG)\n",
        "\n",
        "    # 3. Extract features using trained models\n",
        "    print(\"Extracting features...\")\n",
        "    features_dict = extract_features(data_dict, models_dict, CONFIG)\n",
        "\n",
        "    # 4. Train Random Forest classifier on concatenated features\n",
        "    print(\"Training Random Forest classifier...\")\n",
        "    rf_classifier, metrics = train_random_forest(features_dict, whole_body_dict, CONFIG)\n",
        "\n",
        "    # 5. Save the final model and report results\n",
        "    print(\"Saving model and reporting results...\")\n",
        "\n",
        "    # Save the models_dict (region-wise feature extractors)\n",
        "    torch.save({\n",
        "        'models_dict': {region: model.state_dict() for region, model in models_dict.items()},\n",
        "        'rf_classifier': rf_classifier,\n",
        "        'metrics': metrics\n",
        "    }, os.path.join(save_dir, \"mSegResRF_SPECT_final.pth\"))\n",
        "\n",
        "    # Optionally, save the metrics separately in a text file for future reference\n",
        "    with open(os.path.join(save_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(f\"Accuracy: {metrics[0]:.4f}\\n\")\n",
        "        f.write(f\"Sensitivity: {metrics[1]:.4f}\\n\")\n",
        "        f.write(f\"Specificity: {metrics[2]:.4f}\\n\")\n",
        "        f.write(f\"F1 Score: {metrics[3]:.4f}\\n\")\n",
        "        f.write(f\"AUC: {metrics[4]:.4f}\\n\")\n",
        "\n",
        "    print(\"Training complete and results saved!\")\n",
        "    return rf_classifier, metrics, models_dict\n",
        "\n",
        "# Run the training pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    rf_classifier, metrics, models_dict = train_mSegResRF_SPECT()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZzod_A28xAu",
        "outputId": "e5848db8-5da6-4bed-ca1a-b945907bb4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting mSegResRF-SPECT training pipeline...\n",
            "Preparing data...\n",
            "Found label file for whole body: bs-80k/temp/wholeBodyANT/wholeBodyANT.txt\n",
            "Found label file: bs-80k/temp/headANT/headANT.txt\n",
            "Warning: Region directory bs-80k/temp/vertebraANT does not exist.\n",
            "Found label file: bs-80k/temp/chestLANT/chestLANT.txt\n",
            "Found label file: bs-80k/temp/chestRANT/chestRANT.txt\n",
            "Found label file: bs-80k/temp/pelvisANT/pelvisANT.txt\n",
            "Found label file: bs-80k/temp/kneeLANT/kneeLANT.txt\n",
            "Found label file: bs-80k/temp/kneeRANT/kneeRANT.txt\n",
            "Total number of samples in whole_body_dict: 2925\n",
            "Label distribution:\n",
            "  Label 0: 1860 samples\n",
            "  Label 1: 1065 samples\n",
            "Training feature extractors...\n",
            "\n",
            "Training feature extractor for headANT...\n",
            "Region: headANT, Epoch: 5/100, Train Loss: 0.2417, Val Loss: 0.2317, Val Acc: 0.9265\n",
            "Saving best model for headANT at epoch 5, val_acc = 0.9265\n",
            "Region: headANT, Epoch: 10/100, Train Loss: 0.1997, Val Loss: 0.2447, Val Acc: 0.9043\n",
            "Region: headANT, Epoch: 15/100, Train Loss: 0.1386, Val Loss: 0.2697, Val Acc: 0.9282\n",
            "Saving best model for headANT at epoch 15, val_acc = 0.9282\n",
            "Region: headANT, Epoch: 20/100, Train Loss: 0.1042, Val Loss: 0.3769, Val Acc: 0.9368\n",
            "Saving best model for headANT at epoch 20, val_acc = 0.9368\n",
            "Region: headANT, Epoch: 25/100, Train Loss: 0.0649, Val Loss: 0.4685, Val Acc: 0.8701\n",
            "Region: headANT, Epoch: 30/100, Train Loss: 0.0598, Val Loss: 0.4231, Val Acc: 0.9350\n",
            "Region: headANT, Epoch: 35/100, Train Loss: 0.0452, Val Loss: 0.5282, Val Acc: 0.9231\n",
            "Region: headANT, Epoch: 40/100, Train Loss: 0.0583, Val Loss: 0.4590, Val Acc: 0.9231\n",
            "Region: headANT, Epoch: 45/100, Train Loss: 0.0366, Val Loss: 0.4993, Val Acc: 0.9043\n",
            "Region: headANT, Epoch: 50/100, Train Loss: 0.0295, Val Loss: 0.8252, Val Acc: 0.9333\n",
            "Region: headANT, Epoch: 55/100, Train Loss: 0.0191, Val Loss: 0.4943, Val Acc: 0.9248\n",
            "Region: headANT, Epoch: 60/100, Train Loss: 0.0199, Val Loss: 0.6751, Val Acc: 0.9299\n",
            "Region: headANT, Epoch: 65/100, Train Loss: 0.0144, Val Loss: 0.6530, Val Acc: 0.8803\n",
            "Region: headANT, Epoch: 70/100, Train Loss: 0.0214, Val Loss: 0.5269, Val Acc: 0.9316\n",
            "Region: headANT, Epoch: 75/100, Train Loss: 0.0165, Val Loss: 0.6745, Val Acc: 0.9265\n",
            "Region: headANT, Epoch: 80/100, Train Loss: 0.0134, Val Loss: 0.7748, Val Acc: 0.9197\n",
            "Region: headANT, Epoch: 85/100, Train Loss: 0.0301, Val Loss: 0.4329, Val Acc: 0.9214\n",
            "Region: headANT, Epoch: 90/100, Train Loss: 0.0130, Val Loss: 0.7008, Val Acc: 0.9197\n",
            "Region: headANT, Epoch: 95/100, Train Loss: 0.0115, Val Loss: 0.8201, Val Acc: 0.9265\n",
            "Region: headANT, Epoch: 100/100, Train Loss: 0.0051, Val Loss: 0.6655, Val Acc: 0.9231\n",
            "Finished training for headANT in 423.41 seconds.\n",
            "\n",
            "Training feature extractor for chestLANT...\n",
            "Region: chestLANT, Epoch: 5/100, Train Loss: 0.1082, Val Loss: 0.1395, Val Acc: 0.9641\n",
            "Saving best model for chestLANT at epoch 5, val_acc = 0.9641\n",
            "Region: chestLANT, Epoch: 10/100, Train Loss: 0.0838, Val Loss: 0.1288, Val Acc: 0.9675\n",
            "Saving best model for chestLANT at epoch 10, val_acc = 0.9675\n",
            "Region: chestLANT, Epoch: 15/100, Train Loss: 0.0571, Val Loss: 0.1808, Val Acc: 0.9453\n",
            "Region: chestLANT, Epoch: 20/100, Train Loss: 0.0409, Val Loss: 0.1511, Val Acc: 0.9624\n",
            "Region: chestLANT, Epoch: 25/100, Train Loss: 0.0365, Val Loss: 0.1739, Val Acc: 0.9624\n",
            "Region: chestLANT, Epoch: 30/100, Train Loss: 0.0340, Val Loss: 0.1650, Val Acc: 0.9538\n",
            "Region: chestLANT, Epoch: 35/100, Train Loss: 0.0241, Val Loss: 0.2148, Val Acc: 0.9590\n",
            "Region: chestLANT, Epoch: 40/100, Train Loss: 0.0189, Val Loss: 0.2356, Val Acc: 0.9573\n",
            "Region: chestLANT, Epoch: 45/100, Train Loss: 0.0136, Val Loss: 0.2392, Val Acc: 0.9641\n",
            "Region: chestLANT, Epoch: 50/100, Train Loss: 0.0198, Val Loss: 0.1969, Val Acc: 0.9641\n",
            "Region: chestLANT, Epoch: 55/100, Train Loss: 0.0090, Val Loss: 0.2666, Val Acc: 0.9641\n",
            "Region: chestLANT, Epoch: 60/100, Train Loss: 0.0209, Val Loss: 0.3049, Val Acc: 0.9573\n",
            "Region: chestLANT, Epoch: 65/100, Train Loss: 0.0018, Val Loss: 0.2962, Val Acc: 0.9658\n",
            "Region: chestLANT, Epoch: 70/100, Train Loss: 0.0054, Val Loss: 0.2402, Val Acc: 0.9504\n",
            "Region: chestLANT, Epoch: 75/100, Train Loss: 0.0216, Val Loss: 0.2092, Val Acc: 0.9624\n",
            "Region: chestLANT, Epoch: 80/100, Train Loss: 0.0022, Val Loss: 0.2434, Val Acc: 0.9675\n",
            "Region: chestLANT, Epoch: 85/100, Train Loss: 0.0078, Val Loss: 0.2199, Val Acc: 0.9607\n",
            "Region: chestLANT, Epoch: 90/100, Train Loss: 0.0103, Val Loss: 0.1916, Val Acc: 0.9675\n",
            "Region: chestLANT, Epoch: 95/100, Train Loss: 0.0190, Val Loss: 0.1951, Val Acc: 0.9675\n",
            "Region: chestLANT, Epoch: 100/100, Train Loss: 0.0058, Val Loss: 0.2694, Val Acc: 0.9675\n",
            "Finished training for chestLANT in 423.25 seconds.\n",
            "\n",
            "Training feature extractor for chestRANT...\n",
            "Region: chestRANT, Epoch: 5/100, Train Loss: 0.0876, Val Loss: 0.1156, Val Acc: 0.9538\n",
            "Saving best model for chestRANT at epoch 5, val_acc = 0.9538\n",
            "Region: chestRANT, Epoch: 10/100, Train Loss: 0.0604, Val Loss: 0.1566, Val Acc: 0.9624\n",
            "Saving best model for chestRANT at epoch 10, val_acc = 0.9624\n",
            "Region: chestRANT, Epoch: 15/100, Train Loss: 0.0528, Val Loss: 0.1056, Val Acc: 0.9692\n",
            "Saving best model for chestRANT at epoch 15, val_acc = 0.9692\n",
            "Region: chestRANT, Epoch: 20/100, Train Loss: 0.0529, Val Loss: 0.0764, Val Acc: 0.9726\n",
            "Saving best model for chestRANT at epoch 20, val_acc = 0.9726\n",
            "Region: chestRANT, Epoch: 25/100, Train Loss: 0.0341, Val Loss: 0.1088, Val Acc: 0.9744\n",
            "Saving best model for chestRANT at epoch 25, val_acc = 0.9744\n",
            "Region: chestRANT, Epoch: 30/100, Train Loss: 0.0266, Val Loss: 0.1186, Val Acc: 0.9692\n",
            "Region: chestRANT, Epoch: 35/100, Train Loss: 0.0470, Val Loss: 0.0637, Val Acc: 0.9744\n",
            "Region: chestRANT, Epoch: 40/100, Train Loss: 0.0188, Val Loss: 0.0945, Val Acc: 0.9692\n",
            "Region: chestRANT, Epoch: 45/100, Train Loss: 0.0330, Val Loss: 0.0892, Val Acc: 0.9778\n",
            "Saving best model for chestRANT at epoch 45, val_acc = 0.9778\n",
            "Region: chestRANT, Epoch: 50/100, Train Loss: 0.0081, Val Loss: 0.1948, Val Acc: 0.9675\n",
            "Region: chestRANT, Epoch: 55/100, Train Loss: 0.0127, Val Loss: 0.1299, Val Acc: 0.9726\n",
            "Region: chestRANT, Epoch: 60/100, Train Loss: 0.0051, Val Loss: 0.1010, Val Acc: 0.9778\n",
            "Region: chestRANT, Epoch: 65/100, Train Loss: 0.0164, Val Loss: 0.1136, Val Acc: 0.9709\n",
            "Region: chestRANT, Epoch: 70/100, Train Loss: 0.0081, Val Loss: 0.2699, Val Acc: 0.9538\n",
            "Region: chestRANT, Epoch: 75/100, Train Loss: 0.0330, Val Loss: 0.1355, Val Acc: 0.9590\n",
            "Region: chestRANT, Epoch: 80/100, Train Loss: 0.0012, Val Loss: 0.1311, Val Acc: 0.9692\n",
            "Region: chestRANT, Epoch: 85/100, Train Loss: 0.0184, Val Loss: 0.1276, Val Acc: 0.9709\n",
            "Region: chestRANT, Epoch: 90/100, Train Loss: 0.0090, Val Loss: 0.1214, Val Acc: 0.9778\n",
            "Region: chestRANT, Epoch: 95/100, Train Loss: 0.0069, Val Loss: 0.0996, Val Acc: 0.9761\n",
            "Region: chestRANT, Epoch: 100/100, Train Loss: 0.0096, Val Loss: 0.1450, Val Acc: 0.9658\n",
            "Finished training for chestRANT in 433.03 seconds.\n",
            "\n",
            "Training feature extractor for pelvisANT...\n",
            "Region: pelvisANT, Epoch: 5/100, Train Loss: 0.2119, Val Loss: 0.2209, Val Acc: 0.9299\n",
            "Saving best model for pelvisANT at epoch 5, val_acc = 0.9299\n",
            "Region: pelvisANT, Epoch: 10/100, Train Loss: 0.1671, Val Loss: 0.2228, Val Acc: 0.9368\n",
            "Saving best model for pelvisANT at epoch 10, val_acc = 0.9368\n",
            "Region: pelvisANT, Epoch: 15/100, Train Loss: 0.1272, Val Loss: 0.3194, Val Acc: 0.9197\n",
            "Region: pelvisANT, Epoch: 20/100, Train Loss: 0.1033, Val Loss: 0.3351, Val Acc: 0.9162\n",
            "Region: pelvisANT, Epoch: 25/100, Train Loss: 0.0767, Val Loss: 0.4608, Val Acc: 0.9009\n",
            "Region: pelvisANT, Epoch: 30/100, Train Loss: 0.0547, Val Loss: 0.3684, Val Acc: 0.9282\n",
            "Region: pelvisANT, Epoch: 35/100, Train Loss: 0.0570, Val Loss: 0.3411, Val Acc: 0.9214\n",
            "Region: pelvisANT, Epoch: 40/100, Train Loss: 0.0550, Val Loss: 0.4256, Val Acc: 0.8940\n",
            "Region: pelvisANT, Epoch: 45/100, Train Loss: 0.0409, Val Loss: 0.4089, Val Acc: 0.8564\n",
            "Region: pelvisANT, Epoch: 50/100, Train Loss: 0.0347, Val Loss: 0.3231, Val Acc: 0.9248\n",
            "Region: pelvisANT, Epoch: 55/100, Train Loss: 0.0419, Val Loss: 0.4351, Val Acc: 0.9197\n",
            "Region: pelvisANT, Epoch: 60/100, Train Loss: 0.0294, Val Loss: 0.3950, Val Acc: 0.9231\n",
            "Region: pelvisANT, Epoch: 65/100, Train Loss: 0.0195, Val Loss: 0.5195, Val Acc: 0.9197\n",
            "Region: pelvisANT, Epoch: 70/100, Train Loss: 0.0346, Val Loss: 0.5412, Val Acc: 0.9128\n",
            "Region: pelvisANT, Epoch: 75/100, Train Loss: 0.0210, Val Loss: 0.5511, Val Acc: 0.9350\n",
            "Region: pelvisANT, Epoch: 80/100, Train Loss: 0.0052, Val Loss: 0.5358, Val Acc: 0.9419\n",
            "Saving best model for pelvisANT at epoch 80, val_acc = 0.9419\n",
            "Region: pelvisANT, Epoch: 85/100, Train Loss: 0.0309, Val Loss: 0.5321, Val Acc: 0.9179\n",
            "Region: pelvisANT, Epoch: 90/100, Train Loss: 0.0327, Val Loss: 0.3149, Val Acc: 0.9248\n",
            "Region: pelvisANT, Epoch: 95/100, Train Loss: 0.0170, Val Loss: 0.4190, Val Acc: 0.9197\n",
            "Region: pelvisANT, Epoch: 100/100, Train Loss: 0.0147, Val Loss: 0.4700, Val Acc: 0.9248\n",
            "Finished training for pelvisANT in 429.91 seconds.\n",
            "\n",
            "Training feature extractor for kneeLANT...\n",
            "Region: kneeLANT, Epoch: 5/100, Train Loss: 0.1097, Val Loss: 0.1367, Val Acc: 0.9675\n",
            "Saving best model for kneeLANT at epoch 5, val_acc = 0.9675\n",
            "Region: kneeLANT, Epoch: 10/100, Train Loss: 0.0683, Val Loss: 0.1143, Val Acc: 0.9675\n",
            "Region: kneeLANT, Epoch: 15/100, Train Loss: 0.0606, Val Loss: 0.1173, Val Acc: 0.9624\n",
            "Region: kneeLANT, Epoch: 20/100, Train Loss: 0.0318, Val Loss: 0.2635, Val Acc: 0.9573\n",
            "Region: kneeLANT, Epoch: 25/100, Train Loss: 0.0291, Val Loss: 0.1739, Val Acc: 0.9726\n",
            "Saving best model for kneeLANT at epoch 25, val_acc = 0.9726\n",
            "Region: kneeLANT, Epoch: 30/100, Train Loss: 0.0269, Val Loss: 0.1664, Val Acc: 0.9744\n",
            "Saving best model for kneeLANT at epoch 30, val_acc = 0.9744\n",
            "Region: kneeLANT, Epoch: 35/100, Train Loss: 0.0228, Val Loss: 0.1639, Val Acc: 0.9624\n",
            "Region: kneeLANT, Epoch: 40/100, Train Loss: 0.0176, Val Loss: 0.2391, Val Acc: 0.9658\n",
            "Region: kneeLANT, Epoch: 45/100, Train Loss: 0.0185, Val Loss: 0.2090, Val Acc: 0.9658\n",
            "Region: kneeLANT, Epoch: 50/100, Train Loss: 0.0111, Val Loss: 0.2188, Val Acc: 0.9624\n",
            "Region: kneeLANT, Epoch: 55/100, Train Loss: 0.0203, Val Loss: 0.2788, Val Acc: 0.9692\n",
            "Region: kneeLANT, Epoch: 60/100, Train Loss: 0.0143, Val Loss: 0.2288, Val Acc: 0.9692\n",
            "Region: kneeLANT, Epoch: 65/100, Train Loss: 0.0122, Val Loss: 0.2517, Val Acc: 0.9658\n",
            "Region: kneeLANT, Epoch: 70/100, Train Loss: 0.0103, Val Loss: 0.3189, Val Acc: 0.9692\n",
            "Region: kneeLANT, Epoch: 75/100, Train Loss: 0.0055, Val Loss: 0.1938, Val Acc: 0.9709\n",
            "Region: kneeLANT, Epoch: 80/100, Train Loss: 0.0186, Val Loss: 0.1481, Val Acc: 0.9675\n",
            "Region: kneeLANT, Epoch: 85/100, Train Loss: 0.0107, Val Loss: 0.1526, Val Acc: 0.9692\n",
            "Region: kneeLANT, Epoch: 90/100, Train Loss: 0.0277, Val Loss: 0.2032, Val Acc: 0.9248\n",
            "Region: kneeLANT, Epoch: 95/100, Train Loss: 0.0158, Val Loss: 0.2197, Val Acc: 0.9692\n",
            "Region: kneeLANT, Epoch: 100/100, Train Loss: 0.0063, Val Loss: 0.2368, Val Acc: 0.9675\n",
            "Finished training for kneeLANT in 427.99 seconds.\n",
            "\n",
            "Training feature extractor for kneeRANT...\n",
            "Region: kneeRANT, Epoch: 5/100, Train Loss: 0.1186, Val Loss: 0.1494, Val Acc: 0.9470\n",
            "Saving best model for kneeRANT at epoch 5, val_acc = 0.9470\n",
            "Region: kneeRANT, Epoch: 10/100, Train Loss: 0.0790, Val Loss: 0.1105, Val Acc: 0.9590\n",
            "Saving best model for kneeRANT at epoch 10, val_acc = 0.9590\n",
            "Region: kneeRANT, Epoch: 15/100, Train Loss: 0.0639, Val Loss: 0.3829, Val Acc: 0.8479\n",
            "Region: kneeRANT, Epoch: 20/100, Train Loss: 0.0344, Val Loss: 0.2849, Val Acc: 0.9658\n",
            "Saving best model for kneeRANT at epoch 20, val_acc = 0.9658\n",
            "Region: kneeRANT, Epoch: 25/100, Train Loss: 0.0393, Val Loss: 0.1561, Val Acc: 0.9573\n",
            "Region: kneeRANT, Epoch: 30/100, Train Loss: 0.0135, Val Loss: 0.2365, Val Acc: 0.9590\n",
            "Region: kneeRANT, Epoch: 35/100, Train Loss: 0.0357, Val Loss: 0.2813, Val Acc: 0.9675\n",
            "Saving best model for kneeRANT at epoch 35, val_acc = 0.9675\n",
            "Region: kneeRANT, Epoch: 40/100, Train Loss: 0.0197, Val Loss: 0.2637, Val Acc: 0.9624\n",
            "Region: kneeRANT, Epoch: 45/100, Train Loss: 0.0247, Val Loss: 0.1809, Val Acc: 0.9641\n",
            "Region: kneeRANT, Epoch: 50/100, Train Loss: 0.0137, Val Loss: 0.2178, Val Acc: 0.9573\n",
            "Region: kneeRANT, Epoch: 55/100, Train Loss: 0.0192, Val Loss: 0.2921, Val Acc: 0.9692\n",
            "Saving best model for kneeRANT at epoch 55, val_acc = 0.9692\n",
            "Region: kneeRANT, Epoch: 60/100, Train Loss: 0.0172, Val Loss: 0.2552, Val Acc: 0.9658\n",
            "Region: kneeRANT, Epoch: 65/100, Train Loss: 0.0047, Val Loss: 0.2856, Val Acc: 0.9624\n",
            "Region: kneeRANT, Epoch: 70/100, Train Loss: 0.0190, Val Loss: 0.3062, Val Acc: 0.9573\n",
            "Region: kneeRANT, Epoch: 75/100, Train Loss: 0.0090, Val Loss: 0.2980, Val Acc: 0.9624\n",
            "Region: kneeRANT, Epoch: 80/100, Train Loss: 0.0069, Val Loss: 0.2560, Val Acc: 0.9556\n",
            "Region: kneeRANT, Epoch: 85/100, Train Loss: 0.0082, Val Loss: 0.3809, Val Acc: 0.9658\n",
            "Region: kneeRANT, Epoch: 90/100, Train Loss: 0.0019, Val Loss: 0.3473, Val Acc: 0.9590\n",
            "Region: kneeRANT, Epoch: 95/100, Train Loss: 0.0156, Val Loss: 0.2168, Val Acc: 0.9607\n",
            "Region: kneeRANT, Epoch: 100/100, Train Loss: 0.0144, Val Loss: 0.3577, Val Acc: 0.9607\n",
            "Finished training for kneeRANT in 426.78 seconds.\n",
            "\n",
            "All regions processed in 2564.37 seconds.\n",
            "Extracting features...\n",
            "Extracting features for headANT...\n",
            "Extracting features for chestLANT...\n",
            "Extracting features for chestRANT...\n",
            "Extracting features for pelvisANT...\n",
            "Extracting features for kneeLANT...\n",
            "Extracting features for kneeRANT...\n",
            "Training Random Forest classifier...\n",
            "Found 2925 common samples across all regions.\n",
            "Total samples: 2925\n",
            "Label distribution: {np.int64(0): np.int64(1860), np.int64(1): np.int64(1065)}\n",
            "\n",
            "Training fold 1/5...\n",
            "Fold 1 - Acc: 0.8821, Sen: 0.7089, Spe: 0.9812, F1: 0.8140, AUC: 0.8916\n",
            "Fold 1 completed in 16.34 seconds.\n",
            "\n",
            "Training fold 2/5...\n",
            "Fold 2 - Acc: 0.8838, Sen: 0.7371, Spe: 0.9677, F1: 0.8220, AUC: 0.9004\n",
            "Fold 2 completed in 16.37 seconds.\n",
            "\n",
            "Training fold 3/5...\n",
            "Fold 3 - Acc: 0.8974, Sen: 0.7793, Spe: 0.9651, F1: 0.8469, AUC: 0.9101\n",
            "Fold 3 completed in 16.02 seconds.\n",
            "\n",
            "Training fold 4/5...\n",
            "Fold 4 - Acc: 0.8564, Sen: 0.7136, Spe: 0.9382, F1: 0.7835, AUC: 0.8673\n",
            "Fold 4 completed in 15.74 seconds.\n",
            "\n",
            "Training fold 5/5...\n",
            "Fold 5 - Acc: 0.8855, Sen: 0.7465, Spe: 0.9651, F1: 0.8260, AUC: 0.8870\n",
            "Fold 5 completed in 17.91 seconds.\n",
            "\n",
            "Average metrics - Acc: 0.8810, Sen: 0.7371, Spe: 0.9634, F1: 0.8185, AUC: 0.8913\n",
            "\n",
            "Total training time: 145.86 seconds.\n",
            "Saving model and reporting results...\n",
            "Training complete and results saved!\n"
          ]
        }
      ]
    }
  ]
}